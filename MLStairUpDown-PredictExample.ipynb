{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import pandas for dataframes, import csv, import os for file handling\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) #suppress futurewarnings from matplotlib\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from scipy import stats as st\n",
    "import csv\n",
    "import sklearn\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "global df\n",
    "csv_name = 'ExampleData.csv' #INPUT csv name here\n",
    "df = pd.read_csv(csv_name) #import csv\n",
    "\n",
    "for col in df.columns:\n",
    "    if (('GlobalEfficiency' in col) or ('MaximizedModularity' in col) \n",
    "        or ('MeanClusteringCoeff' in col) or ('MeanTotalStrength'in col)\n",
    "        or ('NetworkCharacteristic' in col) or ('TotalStrength' in col)\n",
    "        or ('dummyrest' in col) or ('session_id' in col) or ('subject_id' in col)\n",
    "        or ('dummy_rest' in col) or ('file_name' in col) or ('1back' in col)\n",
    "        or ('acq_id' in col) or ('anatomical_zstat1' in col) or ('datetime' in col)):\n",
    "        del df[col]\n",
    "\n",
    "df['Sex'].replace(['Female','Male'],[0,1],inplace=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    if(df[col].isnull().values.any()):\n",
    "        if(df[col].isnull().sum()>50):\n",
    "            del(df[col])\n",
    "    \n",
    "df=df.dropna()\n",
    "df = df._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.iloc[:,0:df.columns.size].values\n",
    "# Y = df['fft_stair_us_tester1'].values\n",
    "\n",
    "#https://towardsdatascience.com/3-essential-ways-to-calculate-feature-importance-in-python-2f9149592155\n",
    "#preprocess data, X and Y train/test split, scale x values \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('fft_stair_ds_tester1', axis=1)\n",
    "X2 = df.drop(['fft_stair_ds_tester1','Sex'], axis=1) #no sex\n",
    "y = df['fft_stair_ds_tester1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.25, random_state=42)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)\n",
    "\n",
    "ss2 = StandardScaler()\n",
    "X2_train_scaled = ss2.fit_transform(X2_train)\n",
    "X2_test_scaled = ss2.transform(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection using XGB classifier ------ SEX included\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "importances = pd.DataFrame(data={\n",
    "    'Attribute': X_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "})\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "importances.drop(importances.loc[importances['Importance']<0.007].index, inplace=True)\n",
    "print(importances)\n",
    "plt.bar(x=importances['Attribute'], height=importances['Importance'], color='#087E8B')\n",
    "plt.title('Feature importances, XGB, With Sex')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection using XGB classifier ------ SEX NOT included\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(X2_train_scaled, y_train)\n",
    "importances = pd.DataFrame(data={\n",
    "    'Attribute': X2_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "})\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "importances.drop(importances.loc[importances['Importance']<0.004].index, inplace=True)\n",
    "print(importances)\n",
    "plt.bar(x=importances['Attribute'], height=importances['Importance'], color='#087E8B')\n",
    "plt.title('Feature importances, XGB, Without Sex')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "#feature selection using SelectKBest, f_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "def select_features1(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=f_regression, k='all')\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    " \n",
    "X_train_fs, X_test_fs, fs = select_features1(X_train, y_train, X_test)\n",
    "for i in range(len(fs.scores_)):\n",
    "    if(fs.scores_[i]>10):\n",
    "        print(df.columns[i]+\": \"+ str(fs.scores_[i]))\n",
    "plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "plt.title('Feature importance, KBest (f_reg), with Sex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "#feature selection using SelectKBest, f_regression, sex Not Included\n",
    "def select_features2(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=f_regression, k='all')\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    " \n",
    "X_train_fsNoSex, X_test_fsNoSex, fsNoSex = select_features2(X2_train, y_train, X2_test)\n",
    "for i in range(len(fsNoSex.scores_)):\n",
    "    if(fsNoSex.scores_[i]>10):\n",
    "        print(df.columns[i]+\": \"+ str(fsNoSex.scores_[i]))\n",
    "plt.bar([i for i in range(len(fsNoSex.scores_))], fsNoSex.scores_)\n",
    "plt.title('Feature importance, KBest (f_reg), without Sex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "#feature selection using SelectKBest, mutual_info_regression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def select_features3(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    " \n",
    "X_train_fs2, X_test_fs2, fs2 = select_features3(X_train, y_train, X_test)\n",
    "for i in range(len(fs2.scores_)):\n",
    "    if(fs2.scores_[i]>.175):\n",
    "        print(df.columns[i]+\": \"+ str(fs2.scores_[i]))\n",
    "plt.bar([i for i in range(len(fs2.scores_))], fs2.scores_)\n",
    "plt.title('Feature importance, KBest (MutualInfo), with Sex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "#feature selection using SelectKBest, mutual_info_regression, Sex Not Included\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def select_features4(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    " \n",
    "X_train_fs2NoSex, X_test_fs2NoSex, fs2NoSex = select_features4(X2_train, y_train, X2_test)\n",
    "for i in range(len(fs2NoSex.scores_)):\n",
    "    if(fs2NoSex.scores_[i]>.175):\n",
    "        print(df.columns[i]+\": \"+ str(fs2NoSex.scores_[i]))\n",
    "plt.bar([i for i in range(len(fs2NoSex.scores_))], fs2NoSex.scores_)\n",
    "plt.title('Feature importance, KBest (MutualInfo), without Sex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression, All features (with Sex)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "result = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, result)\n",
    "mse = mean_squared_error(y_test, result)\n",
    "print('Mean Absolute Error:'+str(mae))\n",
    "print('Mean Squared Error:'+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=f_regression, k=10)\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    " \n",
    "model = LinearRegression()\n",
    "model.fit(X_train_fs, y_train)\n",
    "result1 = model.predict(X_test_fs)\n",
    "mae1 = mean_absolute_error(y_test, result1)\n",
    "mse1 = mean_squared_error(y_test, result1)\n",
    "print('MAE1: %.3f' % mae1)\n",
    "print('MSE1: %.3f' % mse1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_fsNoSex, y_train)\n",
    "result2 = model.predict(X_test_fsNoSex)\n",
    "mae2 = mean_absolute_error(y_test, result2)\n",
    "mse2 = mean_squared_error(y_test, result2)\n",
    "print('MAE2: %.3f' % mae2)\n",
    "print('MSE2: %.3f' % mse2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_fs2, y_train)\n",
    "result3 = model.predict(X_test_fs2)\n",
    "mae3 = mean_absolute_error(y_test, result3)\n",
    "mse3 = mean_squared_error(y_test, result3)\n",
    "print('MAE3: %.3f' % mae3)\n",
    "print('MSE3: %.3f' % mse3)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_fs2NoSex, y_train)\n",
    "result4 = model.predict(X_test_fs2NoSex)\n",
    "mae4 = mean_absolute_error(y_test, result4)\n",
    "mse4 = mean_squared_error(y_test, result4)\n",
    "print('MAE4: %.3f' % mae4)\n",
    "print('MSE4: %.3f' % mse4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structural = ['fwhm','snr','cnr','fber','efc','qi1','qi2','icvs','rpve','inu','summary']\n",
    "functional = ['efc','fber','fwhm','ghost_x','snr','dvars','gcor','mean_fd','num_fd','perc_fd','outlier','quality']\n",
    "X_s = X_f = X.copy(deep=True)\n",
    "for col in X_s.columns:\n",
    "    flag = True\n",
    "    for item in structural:\n",
    "        if (item in col):\n",
    "            flag = False\n",
    "    if(flag):\n",
    "        del X_s[col]\n",
    "        \n",
    "X_f = df.copy(deep=True)\n",
    "for col in X_f.columns:\n",
    "    flag = True\n",
    "    for item in functional:\n",
    "        if (item in col):\n",
    "            flag = False\n",
    "    if(flag):\n",
    "        del X_f[col]\n",
    "\n",
    "display(X_s)\n",
    "y = df['fft_stair_ds_tester1']\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X_s, y, test_size=0.2, random_state=42)\n",
    "X_f_train, X_f_test, y_f_train, y_f_test = train_test_split(X_f, y, test_size=0.2, random_state=42)\n",
    "display(X_s_train)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_s_train_scaled = ss.fit_transform(X_s_train)\n",
    "X_s_test_scaled = ss.transform(X_s_test)\n",
    "\n",
    "ss2 = StandardScaler()\n",
    "X_f_train_scaled = ss2.fit_transform(X_f_train)\n",
    "X_f_test_scaled = ss2.transform(X_f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection using XGB classifier ------ structural and functional\n",
    "model = XGBRegressor()\n",
    "model.fit(X_s_train_scaled, y_s_train)\n",
    "importances_s = pd.DataFrame(data={\n",
    "    'Attribute': X_s_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "})\n",
    "importances_s = importances_s.sort_values(by='Importance', ascending=False)\n",
    "importances_s.drop(importances_s.loc[importances_s['Importance']<0.01].index, inplace=True)\n",
    "print(importances_s.head())\n",
    "plt.bar(x=importances_s['Attribute'], height=importances_s['Importance'], color='#087E8B')\n",
    "plt.title('Feature importances, XGB, Structural')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(X_f_train_scaled, y_f_train)\n",
    "importances_f = pd.DataFrame(data={\n",
    "    'Attribute': X_f_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "})\n",
    "importances_f = importances_f.sort_values(by='Importance', ascending=False)\n",
    "importances_f.drop(importances_f.loc[importances_f['Importance']<0.01].index, inplace=True)\n",
    "print(importances_f.head())\n",
    "plt.bar(x=importances_f['Attribute'], height=importances_f['Importance'], color='#087E8B')\n",
    "plt.title('Feature importances, XGB, functional')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "#Regression with tuned number of features - structural\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the pipeline to evaluate\n",
    "model = LinearRegression()\n",
    "fs = SelectKBest(score_func=mutual_info_regression)\n",
    "pipeline = Pipeline(steps=[('sel',fs), ('lr', model)])\n",
    "# define the grid\n",
    "grid = dict()\n",
    "grid['sel__k'] = [i for i in range(X_s_train.shape[1]-(X_s_train.shape[1]-1), X_s_train.shape[1]+1)]\n",
    "# define the grid search\n",
    "search = GridSearchCV(pipeline, grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv)\n",
    "# perform the search\n",
    "results = search.fit(X_s_train, y_s_train)\n",
    "# summarize best\n",
    "print('Best MAE: %.3f' % results.best_score_)\n",
    "print('Best Config: %s' % results.best_params_)\n",
    "print(\"Actual    Predicted\")\n",
    "count = 0\n",
    "for i in y_s_test:\n",
    "    print(str(i)+\"  \"+str(results.predict(X_s_test)[count]))\n",
    "    count+=1\n",
    "# summarize all\n",
    "means = results.cv_results_['mean_test_score']\n",
    "params = results.cv_results_['params']\n",
    "print(\"\\nAll configs:\")\n",
    "for mean, param in zip(means, params):\n",
    "    print(\">%.3f with: %r\" % (mean, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "#Regression with tuned number of features - functional\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the pipeline to evaluate\n",
    "model = LinearRegression()\n",
    "fs = SelectKBest(score_func=mutual_info_regression)\n",
    "pipeline = Pipeline(steps=[('sel',fs), ('lr', model)])\n",
    "# define the grid\n",
    "grid = dict()\n",
    "grid['sel__k'] = [i for i in range(X_f_train.shape[1]-(X_f_train.shape[1]-1), X_f_train.shape[1]+1)]\n",
    "# define the grid search\n",
    "search = GridSearchCV(pipeline, grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv)\n",
    "# perform the search\n",
    "results = search.fit(X_f_train, y_f_train)\n",
    "# summarize best\n",
    "print('Best MAE: %.3f' % results.best_score_)\n",
    "print('Best Config: %s' % results.best_params_)\n",
    "print(\"Actual    Predicted\")\n",
    "count = 0\n",
    "for i in y_f_test:\n",
    "    print(str(i)+\"  \"+str(results.predict(X_f_test)[count]))\n",
    "    count+=1\n",
    "# summarize all\n",
    "means = results.cv_results_['mean_test_score']\n",
    "params = results.cv_results_['params']\n",
    "print(\"\\nAll configs:\")\n",
    "for mean, param in zip(means, params):\n",
    "    print(\">%.3f with: %r\" % (mean, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "#Regression with tuned number of features - functional\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the pipeline to evaluate\n",
    "model = LinearRegression()\n",
    "fs = SelectKBest(score_func=mutual_info_regression)\n",
    "pipeline = Pipeline(steps=[('sel',fs), ('lr', model)])\n",
    "# define the grid\n",
    "grid = dict()\n",
    "grid['sel__k'] = [i for i in range(X_train.shape[1]-(X_train.shape[1]-1), X_train.shape[1]+1)]\n",
    "# define the grid search\n",
    "search = GridSearchCV(pipeline, grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv)\n",
    "# perform the search\n",
    "results = search.fit(X_train, y_train)\n",
    "# summarize best\n",
    "print('Best MAE: %.3f' % results.best_score_)\n",
    "print('Best Config: %s' % results.best_params_)\n",
    "print(\"Actual    Predicted\")\n",
    "count = 0\n",
    "for i in y_test:\n",
    "    print(str(i)+\"  \"+str(results.predict(X_test)[count]))\n",
    "    count+=1\n",
    "# summarize all\n",
    "means = results.cv_results_['mean_test_score']\n",
    "params = results.cv_results_['params']\n",
    "print(\"\\nAll configs:\")\n",
    "for mean, param in zip(means, params):\n",
    "    print(\">%.3f with: %r\" % (mean, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "#Regression with tuned number of features NO SEX\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the pipeline to evaluate\n",
    "model = LinearRegression()\n",
    "fs = SelectKBest(score_func=mutual_info_regression)\n",
    "pipeline = Pipeline(steps=[('sel',fs), ('lr', model)])\n",
    "# define the grid\n",
    "grid = dict()\n",
    "grid['sel__k'] = [i for i in range(X2_train.shape[1]-(X2_train.shape[1]-1), X2_train.shape[1]+1)]\n",
    "# define the grid search\n",
    "search = GridSearchCV(pipeline, grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv)\n",
    "# perform the search\n",
    "results = search.fit(X2_train, y2_train)\n",
    "# summarize best\n",
    "print('Best MAE: %.3f' % results.best_score_)\n",
    "print('Best Config: %s' % results.best_params_)\n",
    "# summarize all\n",
    "means = results.cv_results_['mean_test_score']\n",
    "params = results.cv_results_['params']\n",
    "for mean, param in zip(means, params):\n",
    "    print(\">%.3f with: %r\" % (mean, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual    Predicted\")\n",
    "count = 0\n",
    "for i in y2_test:\n",
    "    print(str(i)+\"  \"+str(results.predict(X2_test)[count]))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "rfe = RFE(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_features_to_select=3,\n",
    ")\n",
    "model = DecisionTreeRegressor()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X_s_train, y_s_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "pipeline.fit(X_s_train, y_s_train)\n",
    "pipeline.predict(X_s_test)\n",
    "# count = 0\n",
    "# for i in y_test:\n",
    "#     print(str(i)+\"  \"+str(pipeline.predict(X_s_test)[count]))\n",
    "#     count+=1\n",
    "for i in range(X.shape[1]):\n",
    "\tprint('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
    "#code pulled from above website to get best model from Logistic, Perceptron, Decision Tree, Random Forest, and Grad. Boost\n",
    "#evaluated using cross validation between different mdoels\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\t# lr\n",
    "\trfe = RFE(estimator=LogisticRegression(), n_features_to_select=5)\n",
    "\tmodel = DecisionTreeRegressor()\n",
    "\tmodels['lr'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\t# perceptron\n",
    "\trfe = RFE(estimator=Perceptron(), n_features_to_select=5)\n",
    "\tmodel = DecisionTreeRegressor()\n",
    "\tmodels['per'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\t# cart\n",
    "\trfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5)\n",
    "\tmodel = DecisionTreeRegressor()\n",
    "\tmodels['cart'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\t# rf\n",
    "\trfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=5)\n",
    "\tmodel = DecisionTreeRegressor()\n",
    "\tmodels['rf'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\t# gbm\n",
    "\trfe = RFE(estimator=GradientBoostingRegressor(), n_features_to_select=5)\n",
    "\tmodel = DecisionTreeRegressor()\n",
    "\tmodels['gbm'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\treturn models\n",
    " \n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    "\n",
    "models = get_models()\n",
    "\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print(scores)\n",
    "    print(name+\" \"+mean(scores)+\" \"+ std(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
